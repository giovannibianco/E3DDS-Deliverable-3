\documentclass[12pt,a4paper]{article}

% \usepackage{array}
% \usepackage{pbox}
% \usepackage{hanging}
\usepackage{fancyhdr}
% \usepackage{caption}
% \usepackage{enumitem}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{url}

\usepackage{xspace}

\usepackage{rotating}
\usepackage{wrapfig}

\usepackage{cite}
\usepackage{textcomp}

\usepackage[gen]{eurosym}
\usepackage{hyperref}

\usepackage{todonotes}

\usepackage{gensymb}

% \usepackage{draftwatermark}
% \SetWatermarkText{draft v2}
% \SetWatermarkScale{6}

\fancypagestyle{plain}{ %
  \fancyhf{} % remove everything
  \renewcommand{\headrulewidth}{0pt} % remove lines as well
  \renewcommand{\footrulewidth}{0pt}
}

\hyphenation{EISCAT}

\input{abbrev}

% \setlength{\topskip}{0mm}
\setlength{\headheight}{15pt}
% \setlength{\topmargin}{-5.4mm}
% \setlength{\textheight}{230mm}
\setlength{\textwidth}{180mm}
\setlength{\oddsidemargin}{-5.0mm}
% \setlength{\evensidemargin}{10.0mm}
% \setlength{\captionmargin}{7mm}

\title{
{\bf Deliverable Document 3} \\
Cluster Management for \ED}
\author{E3DDS Team~\footnote{
Anders Tjulin (EISCAT) {\tt anders.tjulin@eiscat.se};
Ari Lukkarinen (CSC) {\tt ari.lukkarinen@csc.fi};
Assar Westman (EISCAT) {\tt Assar.Westman@eiscat.se};
Carl-Fredrik Enell (EISCAT) {\tt carl-fredrik.enell@eiscat.se};
Dan Johan Jonsson (UiT) {\tt dan.jonsson@uit.no};
Janos Nagy (NSC) {\tt fconagy@nsc.liu.se};
Harri Hellgren (EISCAT) {\tt harri.hellgren@eiscat.se};
Ingemar H\"{a}ggstr\"{o}m (EISCAT) {\tt ingemar.haggstrom@eiscat.se};
Mattias Wadenstein (UmU) {\tt maswan@hpc2n.umu.se};
% Roy Dragseth (UiT) {\tt roy.dragseth@uit.no};
John White (NeIC) {\tt john.white@cern.ch}}}

\date{\today}

\begin{document}

\pagestyle{fancy}
\lhead{\bf E3DDS project}
\rhead{\bf 3: Cluster Management}

\maketitle
\par\noindent
\begin{minipage}{0.5\textwidth}
  \includegraphics[scale=0.18]{NEIC_logo_screen_black.pdf}
  %\vspace{-0.09in}
\end{minipage}
\begin{minipage}{0.5\textwidth}
  \hfill
  %\includegraphics[scale=0.25]{EISCAT3Dlogo1.pdf}
  % New official logo with green text
  \includegraphics[width=0.75\linewidth]{e3d-logo-green-500px}
\end{minipage}

\newpage
\tableofcontents
\newpage

\section{Executive Summary}
\label{exec-summ}

%\todo[inline]{Executive summary to be written at the end of the document writing process? Moved the current exec summary to introduction as that's what it was more like}



\section{Purpose}
\label{purpose}

The purpose of this document is to describe the simulated data processing chain of the upcoming \ED radar.
This document briefly describes each component in the data processing chain and the work required to deploy on the \neinfra{s}.

\subsection{Intended Audience}

The intended audience of this document is primarily the \ED project management and staff in order to understand what is complete and where more work is required in the data chain.
The operators of national \einfra{s} can also use this document to understand the requirements on hardware and networks in order to support \ED.

\section{Introduction}
\label{intro}

The \ED sites, the central transmitting/receiving (TX) and remote receiving (RX) sites, will require some computing capacity.
The amount of computing required at these sites varies according to the capacity and latency of the wide area network (WAN)
that connects them.
Computing clusters to consider fall into these categories:
\bitm
\item Ring Buffer-Beam Former (RBBF) nodes;
\item Prompt computing nodes;
\item Cluster containing Monitoring, Radar Control, Databases, Site login;
\item Local site disk buffers;
\item \ED data archives;
\item User analysis computing nodes.
  \eitm
An assumption is made that the TX site, located at Skibotn NO, will function as the ``central'' site as opposed to the RX sites, nominally located at Kaaresuvanto FI and Kaiseniemi SE.
  
The WAN options are described in~\cite{wan-options}.
From ~\cite{wan-options}, the options are generally ring topology between the TX and RX sites:
\begin{enumerate}
\item IP-routed connections. 200~Gb/s total capacity;
\item Optical Dense Wavelength Division Multiplexing (DWDM). 200~Gb/s per RX site to central site;
\item Optical DWDM with ethernet switches on local sites. 4~Tb/s per RX site to central site;
\item Optical DWDM to central site. 6~Tb/s per RX site to central site.
\end{enumerate}

\section{Site Cluster Management}

Tools for cluster management

\begin{enumerate}
    \item xCat (IBM) open source  Perl, Python, Bash
    \item Ansible (RedHat) open source/enterprice Python, agentless
    \item Puppet open source/enterprice  C++, Clojure, Ruby
    \item Chef open source/enterprice Ruby, Erlang, Domain Specific Language (DSL) 
\end{enumerate}

Software provisioning: OS, Database, VM?, software stack, EasyBuild + CernVM File System

Hardware control: remote console, firmware flash, power management

\subsection{Remote sites}

\subsection{Central site}

OS provisioning, which OS?

Network configuration

Switch configuration


\section{Cluster Login}
\label{sec:cluster}

\subsection{Administrative Login}
\label{sec:admin}

% Here describe the NT1 solution for admin logins...
% 2 factor
% other candidates?

The central (Skibotn) site will host several distinct clusters for \ED.
The Ring Buffer-Beam Former (RBBF) nodes will function as the RAM ring buffer and also produce the narrow angle beams.
These nodes will also write the narrow angle beams into file format to be forwarded to the prompt computing for further processing into data products.
The RBBF cluster is expected to be composed of $30$ high-RAM and throughput computing nodes~\cite{amd-epyc}.
The ``Prompt Computing'' is expected to consist of $N$ commodity computing (optimized for reprocessing of files) or $M$ nodes containing
GPU accelerators.
It is also expected that another cluster, used for administrative purposes, will be sited at Skibotn.
This administrative cluster is not expected to be anywhere near the same scale as the RBBF or Prompt computing clusters
and is a good candidate to be hosted within virtual machines.

The administrators of these clusters will need to be able to access these systems.
This is different to the logins described in Section~\ref{sec:users} as these will be performed by \EC employees or contractors
in order to repair/configure/update the systems.
It is important to remember that these logins will be requesting privileged access (up to root level on Unix-type systems) and contrast to user logins on other \einfra resources.
These administrators may be logging in to the clusters through local terminals or remotely via the internet from any \EC member institution
or contracted partner.
The second case is far more likely.
Some basic requirements for administrative logins can be given:
\bitm
\item Login over secure connections only. i.e. no telnet etc;
\item Logins to clusters restricted to known set of users;
\item Logins and actions traceable (logged);
\item Two-factor authentication~\cite{two-factor} possible;
\eitm

\subsubsection{Bastion hosts}
\label{ssec:bastion}

This subject of securely logging in multiple administrators over many nodes has been studied in industry~\cite{secure-ssh}.
A solution~\cite{fb-ssh} has been implemented by industry leader Facebook using open-source tools.
This approach has been implemented in \nnt, an academic computing \einfra, that shares the same characteristics as the future \ED \einfra.

This scheme is based on X.509 certificate-enabled OpenSSH, an implementation of the industry-standard Secure Shell (SSH) for accessing systems.
This scheme takes advantage of its special features, that are not widely leveraged, to provide both security and reliability using signed certificates with principals.

This solution is discussed in detail in~\cite{fb-ssh}.
The \nnt deployment~\cite{nt1-ssh} also uses the ``Bastion host" concept where the administrator logs in to a highly-protected secured host.
Engineers (administrators) authenticate to the bastion host(s) and obtain the signed SSH certificate.
This X.509 certificate contains all principals (information) allowed for the specific administrator. 
Two-factor authentication can be implemented on the Bastion host to ensure an improved level of strong authentication.
This certificate can now be used to SSH into cluster nodes as either root or other lower privileged users.
In this fashion, no administrator has more access than they requires.

\subsubsection{Local login node}
\label{ssec:local-login-node}

Another scheme for administrators is to deploy (bare metal or virtual) a node, network-local to the \ED clusters, to act as a login node.
The administrator logs in to the login node using SSH.
Once logged in to this node, they can then further login to the cluster(s) via a second SSH step.
The login node can be enabled to require two-factor authentication.

\subsubsection{Direct logins}
\label{ssec:direct}


Directly log in to the nodes on the cluster.
Nodes open to the internet.
root and user access
Maintain a login directory (LDAP or other) to synchronize user name and passwords between nodes.


\subsection{User Login}
\label{sec:users}

The NeIC E3DS project has investigated~\cite{e3ds-md3}
the requirements for users functioning in a scientific (as opposed to administrative) role.
These users need to gain access to the data at the 
Reference mainly to the federated identity e.g. EGI Checkin. Others? EduGAIN federation?

Strong authentication

export controls? 2 factor from the start..



% end of the text of the document...

\newpage
\bibliography{main}{}
\bibliographystyle{unsrt}

\end{document}
